{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List, Union\n",
    "from crewai import Agent, Task, Crew\n",
    "from crewai_tools import BaseTool\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_ollama import ChatOllama\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown,display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Ollama --#\n",
    "llm = ChatOllama(\n",
    "    model=os.environ['llm-model'],\n",
    "    base_url=os.environ['llm-model-api-url']\n",
    ")\n",
    "\n",
    "# -- Cohere API -- #\n",
    "# llm = ChatCohere()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Retrieval Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoutubeSearchTool(BaseTool):\n",
    "    name: str = \"YoutubeV3API\"\n",
    "    description: str = \"YouTube API for Real-Time Video Caption Searching\"\n",
    "    \n",
    "    def _run(self, query: Union[Dict, str]) -> dict:\n",
    "        if type(query) == dict:\n",
    "            for key in query.keys():\n",
    "                if type(query[key]) == str:\n",
    "                    query = query[key]\n",
    "                    break\n",
    "\n",
    "        response = requests.get(\n",
    "            url=os.environ[\"YOUTUBE_API_URL\"],\n",
    "            params={\n",
    "                \"q\": query,\n",
    "                \"type\": \"video\",\n",
    "                \"videoCaption\": \"closedCaption\",\n",
    "                \"key\": os.environ[\"YOUTUBE_API_KEY\"]\n",
    "            }\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        response = response.json()\n",
    "        results = []\n",
    "        for item in response[\"items\"]:\n",
    "            content = YouTubeTranscriptApi.get_transcript(item['id']['videoId'])\n",
    "            raw_text = [part[\"text\"] for part in content]\n",
    "            raw_text = \" \".join(raw_text)\n",
    "            results.append(raw_text)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AresRealTimeSearch(BaseTool):\n",
    "    name: str = \"Ares\"\n",
    "    description: str = \"API for Real-Time Internet Search\"\n",
    "    \n",
    "    def _run(self, query: Union[Dict, str]) -> dict:\n",
    "        if type(query) == dict:\n",
    "            for key in query.keys():\n",
    "                if type(query[key]) == str:\n",
    "                    query = query[key]\n",
    "                    break\n",
    "        payload = { \"query\": [query] }\n",
    "        headers = {\n",
    "            \"x-api-key\": os.environ[\"ARES_API_KEY\"],\n",
    "            \"content-type\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(os.environ[\"ARES_API_URL\"], json=payload, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            return {}\n",
    "        response = response.json()\n",
    "        return response[\"data\"]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = Agent(\n",
    "    role=\"Content Researcher\",\n",
    "    goal=\"Fetch real-time information about the below topic to help compose a helpful response to the actual user query:\\n{topic}\",\n",
    "    backstory=\"Given a user query, your task is to fetch actual facts and information for online resources to compose an insightful knowledge bank. This will be used by other agents to compose a response for the user query. Convert the user query into a string-based search keywords, and use the appropriate tools to fetch relevant information.\",\n",
    "    llm=llm,\n",
    "    allow_delegation=False,\n",
    "    tools=[YoutubeSearchTool(), AresRealTimeSearch()],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_planner = Agent(\n",
    "    role=\"Content Planner\",\n",
    "    goal=\"Given a user query and relevant information from the internet, compose a general structure and outline for a helpful response\",\n",
    "    backstory=\"You are working as a content planner to help users with general purpose queries. You will be provided with real-time information about the user request and you are supposed to generate a table of contents for the writer and editor to compose a helpful response\",\n",
    "    llm=llm,\n",
    "    allow_delegation=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_writer = Agent(\n",
    "    role=\"Content Wrtier\",\n",
    "    goal=\"Given relevant information and an outline, you are required to compose a helpful response for the user query\",\n",
    "    backstory=\"You are the final content writer. You will be provided all relevant information and outline for the response. You are required to use all required inputs to compose a helpful response in a summarized way that answers the user query and provides him with an effective solution\",\n",
    "    llm=llm,\n",
    "    allow_delegation=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_content = Task(\n",
    "    description=(\n",
    "        \"Given the user query: {topic}\\n Convert the user query into relevant search keywords and use the provided tools to search relevant content form the internet and fetch all possible resources to help answer the user query\"\n",
    "    ),\n",
    "    expected_output=\"A list of resources and relevant content that can be useful to answer the user query\",\n",
    "    agent=researcher,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_content = Task(\n",
    "    description=(\n",
    "        \"Given the user query and relvant content, curate a table of content for the response\"\n",
    "    ),\n",
    "    expected_output=\"A comprehensive plan that can be used to curate a well written response\",\n",
    "    agent=content_planner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_response = Task(\n",
    "    description=(\n",
    "        \"Write a detailed and helpful response when provided relevant content and a well-planned table of contents\"\n",
    "    ),\n",
    "    expected_output=\"A well written response that can help user with his query\",\n",
    "    agent=content_writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 18:43:25,723 - 8625070208 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "pipeline = Crew(\n",
    "    agents=[researcher, content_planner, content_writer],\n",
    "    tasks=[research_content, plan_content, write_response],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"topic\": \"Prepare a intro slide on Regression techniques.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [2024-09-04 18:43:27][DEBUG]: == Working Agent: Content Researcher\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 18:43:27][INFO]: == Starting Task: Given the user query: Prepare a intro slide on Regression techniques.\n",
      " Convert the user query into relevant search keywords and use the provided tools to search relevant content form the internet and fetch all possible resources to help answer the user query\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find relevant resources about regression techniques for creating an intro slide presentation.  I'll use YoutubeV3API to find video content related to regression and Ares to search for information on the topic from the web. \n",
      "\n",
      "Action: YoutubeV3API\n",
      "Action Input: {'query': {'title': 'Regression Techniques Intro Slide'}}\u001b[0m\u001b[95m \n",
      "\n",
      "['- WELCOME TO INTRODUCTION \\nTO REGRESSION ANALYSIS. THE GOALS OF THIS VIDEO ARE TO \\nDEFINE REGRESSION ANALYSIS AND ALSO TO DETERMINE HOW TO \\nMEASURE HOW WELL A MODEL FITS GIVEN DATA. WE WON\\'T ACTUALLY PERFORM \\nREGRESSION IN THIS VIDEO. THERE WILL BE SEVERAL OTHER \\nVIDEOS THAT WILL SHOW SPECIFIC TYPES OF REGRESSION. IN STATISTICS, \\nREGRESSION ANALYSIS INVOLVES ANY TECHNIQUES FOR MODELING \\nAND ANALYZING TRENDS BETWEEN A DEPENDENT VARIABLE, OFTEN Y, AND AN INDEPENDENT \\nVARIABLE, OFTEN X. REGRESSION ANALYSIS HELPS US \\nMAKE PREDICTIONS OUTSIDE OF THE GIVEN DATA. IF THE PREDICTION IS WITHIN THE \\nRANGE OF THE GIVEN DATA, IT\\'S CALLED INTERPOLATION. AND IF THE PREDICTION IS OUTSIDE \\nTHE RANGE OF THE GIVEN DATA, IT IS CALLED EXTRAPOLATION. SO FOR EXAMPLE, IF WE HAVE THIS \\nRED SET OF DATA PLOTTED ON THE COORDINATE PLANE, IF WE WANTED TO MAKE A \\nPREDICTION AT LET\\'S SAY X = 1.2, MAYBE SOMEWHERE IN HERE, BECAUSE THIS WOULD BE WITHIN THE \\nRANGE OF THE GIVEN DATA, IT WOULD BE INTERPOLATION. BUT IF WE WANTED TO MAKE A \\nPREDICTION NOW LET\\'S SAY X = 7, MAYBE HERE, SINCE 7 IS OUTSIDE \\nTHE RANGE OF THE GIVEN DATA, IT WOULD BE CALLED \\nEXTRAPOLATION. NOW THIS VIDEO, \\nWE\\'LL USE LINEAR REGRESSION TO INTRODUCE THE CONCEPTS \\nOF REGRESSION AND WAYS OF MEASURING HOW WELL \\nA GIVEN MODEL FITS THE DATA. AS MENTIONED BEFORE, \\nTHERE ARE SEVERAL OTHER VIDEOS THAT WILL FOCUS ON DIFFERENT \\nTYPES OF REGRESSION MODELS. AND THE REGRESSION MODEL CHOSEN \\nREALLY DEPENDS ON HOW THE GIVEN DATA BEHAVES AS WELL AS A CERTAIN NUMBER \\nOF ASSUMPTIONS THAT ARE MADE. AND FOR THIS SET OF VIDEOS, WE\\'LL BE USING THE TI84 \\nGRAPHING CALCULATOR TO PERFORM THE REGRESSION. WE WON\\'T DO IT BY HAND. WHEN ANALYZING DATA, THE FIRST \\nSTEP IS USUALLY TO CREATE A SCATTER PLOT OR SCATTERGRAM. AND THIS IS JUST A GRAPH USING \\nTHE COORDINATE PLANE TO DISPLAY THE VALUES OF THE TWO \\nVARIABLES AS A COLLECTION OF POINTS AS WE \\nSEE HERE AS BLUE SQUARES. AND WHEN CREATING A SCATTER \\nPLOT, IT\\'S IMPORTANT TO REMEMBER \\nTO LABEL THE X OR Y AXES TO KNOW WHAT THEY REPRESENT. SO HERE THE X AXIS \\nREPRESENTS THE PERCENT USING A CONTRACEPTION, AND THE Y COORDINATE \\nREPRESENTS THE TOTAL FERTILITY RATE OF THE GIVEN COUNTRY. SO IN BLUE, WE SEE THE LINE \\nTHAT WAS DETERMINED TO BE THE BEST FIT LINE \\nFOR THE GIVEN DATA. AND WHAT THAT MEANS IS THIS LINE \\nIS SUPPOSED TO REPRESENT THE TREND OF THE DATA, AND THEREFORE WE COULD USE \\nTHE EQUATION OF THIS LINE TO MAKE PREDICTIONS. WHAT I MEAN BY THAT, \\nIF WE KNEW A GIVEN X VALUE OR A COUNTRY\\'S PERCENT \\nUSING CONTRACEPTION, WE COULD MAKE A PREDICTION \\nFOR Y WHICH WOULD TELL US THE TOTAL FERTILITY RATE \\nOF THAT REGION. AND WE COULD ALSO MAKE \\nTHE OPPOSITE PREDICTION. IF WE KNEW THE TOTAL FERTILITY \\nRATE OF A GIVEN REGION, WE COULD MAKE A PREDICTION \\nON THE PERCENT OF PEOPLE USING CONTRACEPTION. SO IN GENERAL, THE IDEA WAS THAT \\nYOU WOULD SKETCH YOUR DATA ON THE COORDINATE PLANE \\nAND THEN MAKE A PREDICTION ABOUT WHAT TYPE OF FUNCTION \\nWOULD BEST FIT THE DATA. SO THE MORE YOU KNOW ABOUT THE \\nBASIC TYPES OF FUNCTIONS, THE BETTER YOU WOULD BE \\nAT DETERMINING WHAT TYPE OF REGRESSION \\nTO PERFORM. AND THERE ARE ALSO WAYS \\nTO DETERMINE HOW WELL A MODEL FITS DATA. SO WHEN WE\\'RE TALKING \\nABOUT LINEAR REGRESSION, THERE\\'S SOMETHING CALLED \\nTHE CORRELATION COEFFICIENT, IDENTIFIED AS A CAPITAL \"R\" \\nOR LOWERCASE \"r,\" AND IT\\'S CALL THE LINEAR \\nCORRELATION COEFFICIENT, AND IT MEASURES THE STRENGTH \\nAND DIRECTION OF THE LINEAR RELATIONSHIP \\nBETWEEN THE TWO VARIABLES. AND THIS IS SOMETIMES \\nCALLED THE PEARSON PRODUCT MOMENT \\nCORRELATION COEFFICIENT. AS YOU CAN SEE HERE \\nBY THIS FORMULA, IT CAN BE QUITE TIME CONSUMING \\nTO CALCULATE, ESPECIALLY IF YOU HAVE A LONG \\nDATA SET WHERE X AND Y ARE THE PAIRS OF DATA \\nAND N IS THE NUMBER OF PAIRS. SO GET A BETTER IDEA OF WHAT \\nTHIS TELLS US ABOUT THE-- SO TO GET A BETTER IDEA OF WHAT \\nTHIS TELLS US ABOUT THE MODEL, LET\\'S TAKE A LOOK AT A FEW \\nEXAMPLES. HERE WE SEE OUR REGRESSION LINE \\nOR LINE OF BEST FIT GRAPHED OVER OUR SCATTER PLOT. THIS IS AN ILLUSTRATION \\nOF A STRONG, POSITIVE CORRELATION \\nBECAUSE THE POINTS ARE TIGHTLY CLUSTERED ALONG \\nA LINE WITH A POSITIVE SLOPE. SO R WOULD BE CLOSE TO A VALUE \\nOF POSITIVE 1. POSITIVE 1 WOULD BE A PERFECT \\nLINEAR FIT WITH A POSITIVE SLOPE. HERE\\'S AN EXAMPLE OF A STRONG \\nNEGATIVE CORRELATION. AND NOTICE THE POINTS \\nARE TIGHTLY CLUSTERED ALONG A LINE \\nWITH A NEGATIVE SLOPE WHICH MEANS THAT R WOULD BE \\nCLOSE TO THE VALUE OF -1. AND IF THERE IS NO CORRELATION \\nAT ALL, WE\\'D HAVE AN R VALUE \\nTHAT\\'S CLOSE TO ZERO AS WE SEE PICTURED HERE. THE SECOND MEASURE OF HOW WELL \\nA MODEL FITS THE DATA IS THE COEFFICIENT OF \\nDETERMINATION OR R SQUARED. R SQUARED MEASURES \\nTHE PROPORTION OF VARIABILITY IN THE DATA SET THAT \\nIS ACCOUNTED FOR BY THE STATISTICAL MODEL. AND THERE IS A FORMULA HERE \\nTO CALCULATE R SQUARED. BUT FOR LINEAR REGRESSION, R SQUARED IS JUST GOING TO BE \\nTHE SQUARE OF R. AND THE CLOSER R SQUARED \\nIS TO 1, THE BETTER THE MODEL. SO IN GENERAL, R SQUARED = 1 - THE SUM OF \\nSQUARES OF THE RESIDUALS DIVIDED \\nBY THE TOTAL SUM OF SQUARES. AND THE SUM OF THE SQUARES \\nOF THE RESIDUALS AS WE SEE HERE IS THE SUM \\nOF THE SQUARES OF THE Y VALUES - THE Y VALUES \\nOF THE MODEL. GEOMETRICALLY, IT WOULD BE THE \\nAREA OF THE SQUARES WE SEE PICTURED HERE. AND THE TOTAL SUMS \\nOF THE SQUARES IS THE SUM OF THE SQUARES OF THE \\nY VALUES - THE AVERAGE Y VALUE. AND WE SEE THOSE VALUES PICTURED \\nHERE IN RED WHERE THIS HORIZONTAL LINE HERE \\nIS Y BAR OR THE AVERAGE Y VALUE. AGAIN TO GET A BETTER FEEL \\nFOR THESE TWO MEASURES, LET\\'S GO AND TAKE A LOOK \\nAT AN ANIMATION. HERE WE HAVE A BLUE SCATTER PLOT \\nAND OUR RED REGRESSION LINE OR LINE OF BEST FIT. RIGHT NOW, R IS EQUAL TO .68 AND \\nR SQUARED IS APPROXIMATELY 47%. SO IT HAS A POSITIVE CORRELATION AND ROUGHLY 47% \\nOF THE VARIABILITY OF THE DATA SET CAN BE EXPLAINED \\nBY THE LINEAR RELATIONSHIP, BY THE EQUATION OF THE LINE. AND WATCH WHAT HAPPENS AS \\nWE INCREASE THE VALUE OF R. THE POINTS BECOME MORE CLUSTERED \\nAROUND THE RED LINE. AND AS WE DO THAT, R INCREASES \\nAND SO DOES R SQUARED. HERE WE HAVE A VERY STRONG \\nPOSITIVE CORRELATION. R = .99 \\nAND R SQUARED = 98.1%. HERE WE SEE A VERY WEAK LINEAR \\nCORRELATION. AND AS WE DECREASE THE VALUE \\nOF R, NOTICE THAT THE SLOPE TURNS \\nNEGATIVE. AND AS R APPROACHES -1, THE POINTS CLUSTER AROUND \\nTHE REGRESSION LINE WITH A NEGATIVE SLOPE. SO HERE WE SEE R = -.99 \\nAND R SQUARED = 98.%. SO EVEN IN THESE EXAMPLES, WE\\'RE \\nLOOKING AT LINEAR EQUATIONS. IF WE LOOK AT DIFFERENT TYPES OF \\nFUNCTIONS, THE CLOSER R SQUARED IS TO 1, THE BETTER THE MODEL FITS \\nTHE DATA. SO I THINK WE\\'LL STOP HERE \\nON THIS VIDEO. AGAIN IN THE NEXT SEVERAL \\nVIDEOS, WE\\'LL TAKE A LOOK AT DETERMINING \\nA VARIETY OF REGRESSION MODELS USING THE TI84 GRAPHING \\nCALCULATOR AND THEN ANSWERING A VARIETY \\nOF QUESTIONS BASED UPON OUR MODEL. I HOPE YOU FOUND \\nTHIS INTRO HELPFUL. THANK YOU FOR WATCHING.', \"okay we're going to talk about statistical learning and and models now i'm going to tell you what what models are good for how we use them and what are some of the issues involved okay so we see three plots in front of us these are sales figures from a marketing campaign as a function of amount spent on tv ads radio ads and newspaper ads and you can see the at least in the first two is a somewhat of a trend and in fact we've we've summarized the trend by a little linear regression line in each and so we see that there's some relationship the first two again look uh stronger than than the third now in a situation like this we typically like to know the joint relationship between the response sales and all three of these together you know we want to understand how they operate together to influence sales so you can think of that as as wanting to model sales as a as a function of tv radio and newspaper all jointly together so how do we do that so before we we get into the details let's set up some notation so yes sales is the response or the target that we wish to predict or model and we usually refer to it as as y we use the letter y to refer to it tv is a is one of the features or inputs or predictors and we'll call it x1 likewise radio is x2 and so on so in this case we've got three predictors and we can refer to them collectively by a vector as x equal to with three components x1 x2 and x3 and vectors we generally think of as column vectors and so that's a little bit of notation and so now in this more compact notation we can write our model as y equals function of x plus error okay and this error um is just a catch all to it captures the measurement errors maybe in y and other discrepancies our function of x is never going to model y perfectly so there's going to be a lot of things we can't capture with the function and that's caught up in the error and again f of x here is now a function of this vector x which has these three arguments three components so what is what is the function f of x good for so with a good f we can make predictions of y at new points x equals little x so this notation capital x equals little x you know capital x we think is the variable having these three components and little x is an instance also with three components particular values for newspaper radio and tv with the model we can understand which components of x in general it'll have p components if there's p predictors are important explaining why and which are relevant for example if if we model an income as a function of demographic variables seniority and years of education might have a big impact on income but marital status typically does not and we'd like our model to be able to tell us that and depending on the complexity of f we may be able to understand how each component xj affects y in in what particular fashion it affects y so models have have many uses and those are amongst them okay well what is this function f and is there an ideal f so in the plot we've got a large sample of points from a population there's just a single x in this case and a response y and you can see there's that's a scatter plot so we see there's a lot of point there's 2 000 points here let's think of this as actually the whole population or rather as a representation of a very large population and so now let's think of what what a good function f might be and let's say not just the whole function but let's think what value would we like f to have at say the value of x equals 4 so at this point over here right we want to query x f at all values of x but we wondering what it should be at the value 4. so you'll notice that at the x equals 4 there's many values of y but a function can only take on one value the function is going to deliver back one value so what is a good value well one good value is to deliver back the average values of those y's who have x equal to 4. and that we write in this sort of mathy notation over here it says the function at the value 4 is the expected value of y given x equals 4 and that expected value is a just a fancy word for average it's actually a conditional average given x equals four since we can only deliver one value of the function at x equals four um the average seems like a good value and if we do that at each value of x so at every single value of x we deliver back the average of the y's that have that value of x so for example at x equals 5 again we want to have the average value in this little conditional slice here that will trace out this little red curve that we have here and that's called the regression function so the regression function gives you the conditional expectation of y given x at each value of x so that in a sense is the ideal function for a for a population in this case of y and a single x so let's talk more about this regression function it's also defined for a vector x so if x has got three components for example it's going to be the conditional expectation of y given the three particular instances of of the three components of x so so if you think about that um let's think of of x as being two dimensional because we can think in three dimensions so let's say x lies on the table two-dimensional x and y stands up vertically so the idea is the same we want to we've got a whole continuous cloud of of y's and x's we go to a particular point x with two coordinates x1 and x2 and we say what's a good value for the function at that point well we're just going to go up in the slice and average the y's above that point and we'll do that at all points in the plane we said it's the ideal or optimal predictor of y with regard for the function and what that means is actually it's it's with regard to a loss function and what it means is that particular choice of the function f of x will minimize the sum of squared errors right which we write in this fashion again expected value of y minus g of x over all functions g at each point x right so it minimizes the average prediction errors now at each point x we're going to make mistakes because if we use this function to predict why because there's lots of y's at each point x right and so the errors that we make we call in this case we call them epsilons and those are the irreducible error you might know the ideal function f but of course it doesn't make perfect predictions at each point x so it has to make some errors but on average it does well for any estimate f hat of x and that's what we tend to do we tend to put these little hats on estimators to show that they've been estimated from data and with so f hat of x is an estimate of f of x we can expand the squared prediction error at x into two pieces there's the irreducible piece which is just the variance of the errors and there's the reducible piece which is the difference between our estimate f hat of x and the true function f of x okay and that's a squared component so this expected prediction error breaks up into these two pieces so that's important to bear in mind so if we want to improve our model it's this first piece the reducible piece that we can improve by maybe changing the way we estimate f of x okay so that's all nice this is a kind of as up to now has been somewhat of a theoretical exercise well how do we estimate the function f so the problem is we can't carry out this recipe of conditional expectation or conditional averaging exactly because at any given x in our data set we might not have many points to average we might not have any points to average in the figure i've we've got a much smaller data set now and we've still got the point x equals four and if you look there you'll see carefully that the solid point is one point i put on the plot the solid the solid green point there's actually no data points whose x value is exactly four so how can we compute the conditional expectation or average well what we can do is relax the idea of at the point x to at in a neighborhood of the point x and so that's what the notation here refers to n of x or script n of x is a neighborhood of points defined in some way around the target point which is this x equals four year and it keeps the spirit of conditional expectation it's close to the target point x and if we make that neighborhood wide enough we'll have enough points in the neighborhood to average and we'll use their average to estimate the conditional expectation so this is called nearest neighbor or local averaging it's a very it's a very clever idea it's not my idea it's been invented long time ago and of course you'll move this neighborhood you'll slide this neighborhood along the x-axis and and as you up as you compute the averages as you slide in along it'll trace out a curve so that's actually a very good estimate of the of the of the function f it's not going to be perfect because the the the little window it has a certain width and and so some as we can see here some points of the true f may be lower and some points higher but on average it does quite well so we have a pretty powerful tool here for estimating this conditional expectation just relax the definition and compute the the nearest neighbor average and that gives us a fairly flexible way of fitting a function we'll see in in the next section that this doesn't always work especially as the dimensions get larger and we'll have to have ways of dealing with that\", \"- [Instructor] My name is Brandon and welcome to the next video in my series on basic statistics. If you are new to the channel, welcome, and it's great to have you. If you are a returning viewer, it's great to have you back as well. If you like the video,\\nplease give it a thumbs up, share it with classmates,\\ncolleagues, or friends, or anyone else you think\\nmight benefit from watching. So now that we're introduced, let's go ahead and get started. So this video is the next in our series on nonlinear regression. In previous videos, we talked about why a nonlinear model might\\nbe better than a linear one, we talked about the trade-offs\\na bit in those two options. In the previous video, we talked about general patterns in the quadratic model. Now, in this one, we're gonna talk about the idea of overfitting. And the concept of overfitting, again, has a broad range of application to not only in nonlinear regression but also in other things\\nin machine learning and things like that. But if you get this basic\\nidea of what overfitting is, how it can be a trap that we fall into, you'll better understand\\nthose more advanced things. So, let's go ahead and get started. So let's use a metaphor first. Let's say you go to a shop. There's a seamstress or a tailor and you have that person\\nmake a piece of clothing like a suit that perfectly\\ncontours and fits your body. So you walk out with a suit\\nthat just fits perfectly, it's made just for you,\\nand it's wonderful. Well, let's say in a few\\nyears, you lose some weight. Or maybe you're not done growing yet. Or maybe you gained some weight. Or maybe you're tired of that suit and you give it to someone else. So what problem might you face in that? Well, if your body changes\\nshape, your suit doesn't. So that perfectly fitted suit\\nmay no longer be any good. If you give that suit to someone else, the chances that their body is\\nshaped exactly like your body is probably very small. All of us are shaped\\ndifferently, different sizes, heights, weights, and so forth. So you have a suit that fits you, at one point in your life, perfectly. But when you go to apply it to a different point in your life, or someone else, it may not fit. And that's a very good\\nmetaphor for overfitting. So we can create a model for\\nour data that fits so well that when we throw new\\ndata at it, it falls apart. And that is the risk and\\nthe danger of overfitting. And this is a way to think about that as you progress through statistics. Think about it as a suit\\nthat you have made for you and just for you, that may\\nnot be good for someone else or yourself at a different point in life. So here's the example we've been using. I'm not gonna read through it. We have a car owner that\\nkeeps track of sales data for the salespeople and how\\nmany cars they sell over time based on the amount of\\nweeks they've worked there. And the goal is to produce a\\nmodel that minimizes error. And this is very important. Producing a model that minimizes error, but will also be good for\\nnew data that we throw at it. So here is our original data, here is a scatterplot of that data, and you see how it looks. And we determine that a quadratic model, or a second-order polynomial,\\nactually worked best. So we took that weeks on the job variable, our independent variable, we squared it, created a second variable\\nbased on the square of that, and this was our second order\\npolynomial or our quadratic. So we go ahead and perform\\nthat analysis in Excel, we come out with some output. So our R squared was much\\nhigher than the linear model. Our standard error was much\\nlower than the linear model. The mean square error was about half what it was in the linear model. And we can see that the P-value for both terms down here at\\nthe bottom are significant. So the weeks on the job\\nand the squared term are both significant so we know they contribute\\nto the overall model. And this is good, this is what we want. So we have higher variant\\nexplained, lower standard error, we have half the mean\\nsquare error we had before, and both terms are significant. So this is great, this\\nis exactly what we want. So we can graph those in the same graph and see it obviously\\nthat the red dashed line, which is the quadratic model, fits the data better than\\nthe black dashed line, which is the linear model. So you can say that the R squared value is much higher in the quadratic model so more variance is explained. Again, this is exactly what we want. However, we begin to think, hmm. Well, I did a squared term\\nand that turned out well. So why not just go ahead and\\nthrow another term in there, throw the cube in there? So we're gonna take the weeks on the job and we're gonna cube it. So we're gonna take it to the third power. And this is what we have. So we have job tenure in weeks, then we have the squared term\\nwe had in the quadratic model, now we throw in a third term, which is the cube of\\nthe job tenure in weeks. So now we have those three variables. So here is our third degree polynomial, or our cubed version of this model. So our R square is, you know, 90, .9152. Our standard error is about 32. Those actually sound pretty\\nsimilar to the quadratic, or the second order polynomial. Our mean square is 1066,\\nwhich seems about the same as the other one before. Look at our coefficients down here. We have P-values that, I don't know, are starting to worry me. If you look down here,\\nthe original coefficient is now not significant. The squared term is not significant. The cubed term is not significant. And that should probably\\nraise some suspicion in us when we're looking at this model. So here is the graph of that. So it looks fairly similar\\nto the quadratic model. So we start in the lower left, it kind of goes up on the hill and goes down over to the right. It actually doesn't look all\\nthat much different on a graph. Okay, so that's our\\nthird degree polynomial. So now we start thinking,\\nhmm, that worked out, I guess, it looks pretty good and the\\nterms are about the same, so what if we do another one? Let's throw in a weeks on the job to the fourth power in there. Okay, so now we take our job tenure, we have the square of it,\\nwe have the cube of it, and then we take it to the fourth power. Now we have four variables in the model. Here's the output. We have an R square of .928. We have a standard error of 31.55. Our mean square is 99568. But look at our P-value\\nfor our terms down here. So the job tenure in weeks,\\nthe first one, is .07. That's close, but not significant, because close doesn't count in stats. It's either under .05 or not. And it's not, so it's not significant. Neither are the other three. So again, that should give us pause. And here's the graph of our\\nfourth degree polynomial. So you can see that some\\nweird things start to happen. Here in the lower left,\\nit starts bending down towards the beginning. There's actually a slight\\ndip here in the middle that's, you can't perceive it very well, but it's actually there. And then, of course, it goes\\nup and down to the right. So you see what's happening here? This line, the more terms we add, this line is starting to bend and contort to fit these individual\\nparticular values closer, to fit it better. So it's starting to bend and\\nshift in certain places to fit. So now we're really going to go crazy. Well, that looked okay,\\nlet's add another one. So now we'll take our job\\ntenure to the fifth power. So we have our first\\nfour, and now we're gonna take it to the fifth. So now we have all five\\nvariables in there. We do the fifth degree polynomial. R squared is still high, .9282. Standard error is 33.23, that's\\nabout the same it has been. The mean square is 1104,\\nthat's in the ballpark. Then the P-value, though,\\nlook at everything down here. Nothing is significant anymore. Not even the intercept or nothing. It's all not statistically significant. And here's the graph. So now again, it starts to\\nbend and shape and contort. It's not all that different\\nthan the fourth degree, but you can see it is starting\\nto twist a little bit here, twist a little bit there, as it tries to follow the points exactly. So what we're seeing\\nhere in the graph is that it's trying to bend\\nitself to literally follow each individual point in our dataset. That's what it's trying to do. So what about the model comparison? We want to look at the\\nvalues for the following: the multiple R, the R square,\\nthe adjusted R square, the standard error, and\\nthe mean square error. And we have some questions. How does each value change\\nacross all five models? What are we gaining, if anything, by adding higher order polynomials? And what is happening to\\nthe error across the models? Again, we want to reduce error. So first, multiple R across models. So model one, we had\\na multiple R of .8955. And then it jumped up\\nsignificantly with the quadratic, or the second order polynomial, to .95. That's a pretty big increase. Then, for the third\\norder, it went up a bit, but it was about the same as\\nthe second order polynomial. Here it went up a little bit more. And then it was about the same. You can see that the biggest\\njump was from the linear model, which is the first one,\\nto the quadratic one, which is the second one. And then from there, it\\nwent up a little bit, but it was fairly consistent,\\n.95, .95, .96, .96. Okay, just a little bit of\\nvariation as you go across. So what about the R square? So we have .80, then a big jump\\nwhen we go to the quadratic. And then .91, .92, .92. So that there's a slight\\nincrease as you progress, but again, the biggest jump\\nwas from the linear model to the quadratic model. And then from there, it kind\\nof went up incrementally. And actually, there's actually something very important there,\\nbut I'll get to it in time. What about the adjusted R square? So again, same pattern. The biggest jump was from\\nthe linear to the quadratic, then we have a little bit\\nof fluctuation from there. What about the standard error? So here, we went from 45.94 down to 32.67. That's exactly what we want. So remember, the standard\\nerror is a measure of how well the data points fit\\naround the regression line, whether it's linear or curved. And a decrease is a good thing. That means the data fits\\naround the line better. So here we went 32, 31.5, 33. Again, the same pattern. After you go to the quadratic, it pretty much stays the same across. What about mean square error, MSE? Again, a measure of our model error. So 2110, then it goes down by almost half to 1067 in the quadratic. And then it's about the same. Little bit of dip there in\\nthe fourth order quadratic, but it's about the same. So what about model comparison here? Here they are back to back. So you can see, across all of these, that it would appear we go\\nthe biggest bang for our buck in the quadratic one, which\\nis the second model there. From that, pretty much\\neverything stays the same. Now, overfitting. So as is apparent, by introducing more complexity into our model we're not getting much more out of it. It is a case of diminishing returns. So we can keep throwing in higher and higher order polynomials, but we're not getting more\\nout of it by doing that. All we're doing is\\nover-complicating the model. The model begins to follow the\\npoints in the data exactly. So the more terms we throw at it, the curve, or the line, starts\\nto bend and shift and contort to follow each data point individually. So there we start modeling the natural variation in the data. It actually starts following\\nthe individual points. So we start to have a, back\\nto our original metaphor, a tailored suit problem that\\nwill not fit someone else. So we're creating a\\nmodel that fits exactly, or it's working towards\\nfitting the data exactly, which will not work for something else. So new data may not fit the model because the model is too specific, it's too tailored to this sample. And then, of course, the\\nmachine learning applications, when we validate this model, you know, hey, you know, it really,\\nreally fits tight. Then we throw some new data at it and the errors are all over the place because we've overfit the model. And that's a huge problem. So it's always about a balance, right? You're trying to find that sweet spot. So new data may not fit\\nthis model if we overfit it. So again, when we throw\\nnew data at the model, the error may be high. So how do we choose a model? When we look at this, we can see that the second order of the quadratic model seems to where we get the most value out of adding that extra term. Everything beyond that is just\\nover-complicating the model, weird things start happening in there, and we should just stop. So by looking at these\\nhigher order models, we know that, you know what, we're not getting more out of\\nit the more terms we throw in, so our quadratic model\\nis probably the best. So one thing about our metaphor\\nthat could be good is that if your tailored suit no longer fits you because you've lost some weight\\nand gotten a bit healthier, that's a good thing. So feel free to hand that\\ntailored suit off to someone else who may get some good use out of it. So that wraps up our next\\nvideo in nonlinear regression about a very important topic,\\nthe basics of overfitting. So again, we talked about how\\nmaking a model more complex doesn't mean we get\\nanything more out of it. And in fact, we can have a\\nproblem where throwing new data at an overly tailored\\nor overly fitted model can create problems for new\\ndata we introduce into it. And we did not do this in\\nthis video because again, this was just sort of the beginning of the concept of overfitting,\\nbut we will get there. We're working slowly towards\\nthis idea of throwing new data at a model to see the\\nbalance between fitting and sort of being generalizable. So I hope you enjoyed this video. Thank you for watching. Keep on learning and I look forward to seeing\\nyou again in the next one. Take care.\", \"- [Brandon] Now as I'm sure\\nyou know and experience, the world is a very complex place. So when we're looking to predict the value of a variable, oftentimes we\\ncan get better predictions if we use more than one other variable to make that prediction and that leads us to multiple regression. Now I am going to assume\\nyou have some familiarity and some comfort with\\nsimple linear regression which we covered in the previous series. So if you're still a\\nbit shaky on just simple linear regression, I\\nwould go back, review that and then come back to this series. So without further ado, let's go ahead and get to learning. So as always, let's\\nstart out with a problem and a dataset that we're gonna use for the next several videos. And this is called the\\nregional delivery service. So let's assume that you\\nare a small business owner for Regional Delivery\\nService, Incorporated, or RDS for short, who offers\\nsame-day delivery for letters, packages, and other small cargo. You are able to use Google Maps to group individual\\ndeliveries into one trip to reduce time and fuel\\ncosts, just like UPS would or FedEx or the Postal Service does. Therefore some trips will\\nhave more than one delivery. Now as the owner, you would\\nlike to be able to estimate how long a delivery will\\ntake based on two factors, one, the total distance\\nof the trip in miles and two, the number of\\ndeliveries that must be made during that trip. So we're looking to estimate\\nhow long a delivery trip will take based on the\\ndistance and the number of deliveries during that\\ntrip, so two factors. So to conduct your analysis\\nyou take a random sample of 10 past trips and record\\nthree pieces of information for each trip, one, the\\ntotal miles traveled, two, the number of\\ndeliveries during that trip and three, the total travel time in hours, which is what we're trying to predict. So you make a table that looks like this. So we have miles traveled, num deliveries, which is the number of deliveries, and then travel time\\nin hours along the top. We have labeled them X1, X2 and Y. Now X1 and X2 are special\\ntypes of variables we'll discuss here in a minute. And Y is a distinct variable,\\nwe'll talk about here in a minute. So we can see the first\\ntrip we traveled 89 miles. We had four deliveries on that trip, and the total time was seven hours. The second trip was 66 miles. We only had one delivery and\\nthe travel time was 5.4 hours. So here are our 10 trips. Now remember that in this\\ncase, you would like to be able to predict the total travel\\ntime, so that's the right column so in the orange-brown color there, using both the miles traveled,\\nwhich X1, the first column, and number of deliveries,\\nwhich is the second column X2 of each trip. So the question is, in what\\nway does travel time depend on the first two measures, miles traveled and number of deliveries. So travel time is the dependent variable and miles traveled and\\nnumber of deliveries are independent variables. Now one note here, some\\nprefer predictor variables and response variable\\ninstead of independent and dependent variables respectively. Since most stats textbooks\\nuse independent variable and dependent variable, I\\nam going to stick to that. But I do subscribe to the\\ncase of predictor variables and response variable. Just keep in mind that\\ndepending on the textbook you're using and your\\nprofessor and things, you may hear both or one or the other. So what about multiple regression? So multiple regression\\nis just an extension of simple linear regression,\\nagain which we talked about in the last series. So remember in simple linear\\nregression, we have a one to one relationship. So we have a dependent\\nvariable and we're going to utilize an independent\\nvariable to explain the variation in that dependent variable or make predictions about\\nthat dependent variable. Now in multiple regression, we have a many to one relationship. So we still have one dependent variable but we can have two or more. So in this case we have four on the screen but we can have more than that or just two or three or whatever independent variables that are all being utilized\\nto explain the variation or predict the value of\\nthe dependent variable. So we go from a one to one relationship, one independent to one\\ndependent to two or more independent variables and\\none dependent variable. Now having more independent\\nvariables complicates things a bit. So we have to have some\\nnew things to consider. The first is that adding\\nmore independent variables to a multiple regression\\nprocedure does not mean or necessarily mean the\\nregression will be better or offer better predictions. In fact, doing so can\\nactually make things worse. This is called overfitting. So let's say we conduct a\\nmultiple regression procedure and our model explains\\n65% of the variation in the dependent variable. Well for some reason we don't like that. We think well, we can do better than that. So we start adding in more\\nindependent variables. Well adding more independent\\nvariables will explain more of the variation in\\nthe dependent variable but it can do so under false pretenses. So adding more variables\\nwill always explain more variation, but it can\\nopen up a whole Pandora's box of other problems that we\\ndefinitely want to avoid. So we'll talk about that more as we go, but I just wanna float it out there that dumping more variables\\ninto a multiple regression procedure is not the way to go. The idea is to pick the best\\nvariables for the model. We'll talk about how to\\ndo that in future videos. The other concept is that the addition of more independent\\nvariables, see a pattern here, creates more relationships among them. So not only are the independent\\nvariables potentially related to the dependent\\nvariable, they are also potentially related to each other. Now when this happens, it\\nis called multicollinearity. Now it's a mouthful of a word to say and I stumble it over it\\nsometimes but hopefully we'll get better at it as we go. So it's called multicollinearity when the independent variables are\\ncorrelated with each other. So the ideal, the perfect\\nworld is for all the independent variables to\\nbe correlated with the dependent variable but\\nnot with each other. And again, we'll talk about\\noverfitting more as we go. We'll talk about multicollinearity\\nmore as we go forward and just keep in mind that the ideal is for the independent\\nvariables to be correlated with dependent variable\\nbut not with each other. Now because of multicollinearity\\nand overfitting, there is a fair amount of prep work to do before conducting multiple\\nregression analysis if one is to do it properly. And in a future video,\\nwe will walk through all those things step\\nby step so that you form the best model you can. So things like\\ncorrelations, scatter plots, and some simple regressions\\nbetween the independent variable each one of them, and\\nthe dependent variable just to see how they're related. So to do multiple regression\\nproperly, really running the multiple regression\\nis the very last step. There's a lot of prep work\\nto do before doing that and again we'll talk about it as we go. So as we talked about before,\\nadding more independent variables creates more relationships among all the variables. So we have this many-to-one relationship. Now in our problem we\\nhave a dependent variable that is the travel time. We are trying to predict the travel time of these trips. Now we are utilizing two\\nindependent variables that we selected. We have miles traveled, that's our X1 and then we had the number of deliveries or num deliveries which is our X2. Now we'd like to utilize those\\ntwo independent variables to make predictions about\\nthe dependent variable. Now by setting it up\\nthis way, we also create a third relationship, and\\nthat is the relationship between the two independent\\nvariables themselves. So we don't have just two\\nrelationships, independent and dependent. We now have a relationship\\nbetween the independents. And having that relationship\\nsets up the potential multicollinearity risk. So we're gonna have to see\\nwhen we do this problem, whether or not these two\\nindependent variables are correlated with each other. And the easy way to\\nthink about this is that if these two independent\\nvariables are related to each other, we're\\nreally not sure which one is explaining the variation\\nin the dependent variable. So if I put sea salt and\\ntable salt in my dinner, all I know is that it tastes salty. But I can't tell the\\ndifference necessarily between the two because they're both salt. They have the same relationship\\nto my now salty dinner. So we wanna have a distinction between the independent variables so that it explains something different. We have a different relationship\\nwith the dependent variable over here on the right. So we will walk through that\\nanalysis as we go forward. Now let's look at this situation. So here we have one dependent variable and four independent variables. So we know we have the four relationships with each independent variable\\nand the dependent variable. So right there we already have\\nfour variable relationships. But we're not done. We have to account for\\nall the relationships between the independent variables. And that's six more. So now with four independent variables and one dependent variable,\\nwe have 10 relationships we have to consider. Now you can see as each\\nindependent variable is added, these relationships become very numerous. So part of the art of multiple\\nregression is deciding which independent variables make the cut and which do not. And we'll talk about\\nthat as we go of course. So the bottom line is that\\nsome independent variables or sets of independent\\nvariables are better at predicting the dependent\\nvariable than others. And some independent\\nvariables contribute nothing. So we'll have to decide\\nwhich independent variables to include in our model\\nand which ones to exclude. So again, the ideal is for all\\nof the independent variables to be correlated with\\nthe dependent variable, so the orange lines,\\nbut not with each other, so the colored dotted lines here. So this slide is not something\\nyou have to really commit to memory, but I just wanna\\nshow you sort of where the multiple regression model comes from. So we have our multiple regression model which is Y equals beta\\nsub zero, plus beta one X1 plus beta two, X2 plus et\\ncetera, et cetera, et cetera. P just means the number\\nof variables we have plus epsilon. Now over here on the left\\nwhat we have are the sum of linear parameters. So if beta sub zero,\\nwhich is our intercept and then we have beta one,\\nX1, which is one variable and its weight, then we have X2, beta two, which is another variable and its weight, et cetera, et cetera. So it's just the sum of\\nsome linear parameters. But over here on the right\\nwe have our error term. So we've seen this before\\nin simple linear regression. So we have an intercept plus\\na set of linear parameters plus an error term. Now for the multiple regression equation, we have the expected value\\nof Y equals everything we see up at the top but\\nthere is no error term. Well why is that? That's because in the\\nmultiple regression equation, the error term is assumed to be zero. So zero is zero and\\ntherefore it's not on the end of that equation. The one we're gonna be\\nfamiliar with is the estimated multiple regression equation. So again when we're using sample data, it's never gonna be perfect. We're estimating, so we\\nhave to use the estimative multiple regression equation. So Y hat is the predicted value of Y equals B sub zero plus B1\\nX1 plus B2, X2, et cetera. As you can see it follows the same form as the multiple regression\\nequation above it and everything that you\\nsee at the bottom is just an estimate of what is above it. So B zero, B one, B two\\nare all the estimates of beta zero, beta one,\\nbeta two, et cetera and then Y hat is the predicted value of the dependent variable. So again, this is not\\nsomething you need to really commit to memory, but I just\\nwant you to see the pattern of how these multiple regression equations are gonna look and we'll\\ntalk about in the next slide sort of what they mean. So let's go ahead and look at an example. So this is a multiple regression equation you may generate based on\\nsome analysis you conduct. So if Y hat equals 6.211\\nplus 0.014 X1, plus 0.383 X2, minus 0.607 X3. This is a standard form of a\\nmultiple regression equation you may generate. Now if we look at our estimated\\nmultiple regression equation so we have Y hat equals B zero plus B1, X1 plus B2, X2, et cetera,\\nif you look at that and compare it to the equation at the top, you can see that they're very similar. We just have some stand\\nin numbers that we have to interpret. So we have our variables, so X1, X2 and X3 are our variables and we can\\nsee that they're in place there at the top and of\\ncourse in the equation at the bottom. Then we have some\\ncoefficients and an intercept. So 6.211 is our intercept\\nwhich corresponds with B sub zero in the equation below. Then we have .014 there in\\nthe blue that corresponds to the first coefficient on the bottom, et cetera, et cetera. So we follow the same basic form. Intercept plus some coefficients\\npaired with a variable. So a coefficient with our first variable, a coefficient with the second variable, in this case a coefficient\\nwith the third variable. And again, these are all estimates of the multiple regression model. So how do we interpret the coefficients in multiple regression? They're interpreted a bit differently than they are in simple linear regression. Let's take this example. So we have Y hat equals\\n27 plus 9X1 plus 12X2. Everything we see here is\\nin thousands of dollars. So X1, that's our first\\nvariable stands for capital investment in the thousands of dollars. So X2 stands for the\\nmarketing expenditures in thousands of dollars. That's there in the blue. And of course Y hat is\\ngonna be our predicted sales in thousands of dollars. So everything is in thousands of dollars. We have to keep that\\nin mind as we go about interpreting it. So in multiple regression\\neach coefficient, so we have our nine and our 12 up there, is interpreted as the\\nestimated change in Y corresponding to a one\\nunit change in a variable when all other variables\\nare held constant. So what does that mean in this problem? So in this example, $9,000 is an estimate of the expected increase\\nin sales, which is Y, corresponding to a $1,000\\nincrease in capital investment which is our X1 up there. So remember, everything's\\nin thousands of dollars so I was making sure\\nit's actually in dollars here at the bottom. So $9,000 is an estimate\\nof the expected increase in sales corresponding to a 1,000 increase in capital investment, which is X1. Well why's that? With the big coefficient,\\nwith our X1 variable up there, it is nine. So if we increase X1 or\\nX1 is the number one, we have nine times one, so\\nthat's nine times $1,000. That's $9,000, assuming\\nwe hold the X2 over here on the right constant. And that's how we\\ninterpret the coefficients in multiple regression. We could flip that and say\\nwell, $12,000 is an estimate of the expected increase\\nin sales Y, corresponding to a $1,000 increase in\\nmarketing expenditures when capital investment is held constant. So a one unit increase\\nwhen everything else, all the other variables are held constant. And again, we'll be doing\\nthis more in future videos so we'll get some practice at it. But that's the basic idea of\\nhow we interpret coefficients in a multiple regression equation. Let's go ahead and do a quick review and then we'll be done\\nwith this first video. So multiple regression is an extension of simple linear regression. Two or more independent\\nvariables ar used to predict or explain the variance\\nin one dependent variable. Two problems may arise\\nhowever, overfitting and multicollinearity. So overfitting is caused\\nby adding too many independent variables. They account for more\\nvariance but really add nothing more to the model. When multicollinearity happens when some or all the independent\\nvariables are correlated with each other and it\\nbecomes hard to tell which is actually predicting\\nor explaining the variance in the dependent variable\\n'cause they're so similar. In multiple regression, each\\ncoefficient is interpreted as the estimated change in\\nY, the dependent variable, corresponding to a one\\nunit change in a variable when all other variables\\nare held constant. And again, we'll practice\\nthat more as we go. So this was our first\\nvideo, just the very basics of multiple regression. We'll be doing more videos in the future and walking through the\\nprocess by which we examine our variables. We look at relationships\\namong them before we even ever get into conducting\\nthe multiple regression using a statistics package like SPSS or R or Excel or whatever. There's a lot of pre work\\nto do and that's what we're gonna cover in the next video. So I hope you found this\\nfirst video helpful. If you like the video,\\nplease give it a thumbs up, subscribe, share it or\\nwhatever you wanna do, spread the word. I just do these to help people learn. So I hope you enjoyed it\\nand I'll see you again in the next video. (light music)\", 'There will always be uncertainty in our estimates\\nof the slope and y-intercept for a regression line, just like there is uncertainty in all\\nestimates. This video explores the topic further by accompanying\\nOpenIntro Statistics\\' discussion of \"Inference for Linear Regression\". We will learn about inference for regression\\nparameters using an example based on Midterm elections and unemployment, where we will\\nfocus on understanding regression output from software. Elections for members of the United States\\nHouse of Representatives occur every two years, coinciding every four years with the U.S.\\nPresidential election. The set of House elections occurring during\\nthe middle of a Presidential term are called midterm election. In America\\'s two-party system, one political\\ntheory suggests the higher the unemployment rate, the worse the President\\'s party will\\ndo in the midterm elections. To assess the validity of this claim, we compiled\\nhistorical data to look for a connection. We consider every midterm election from 1898\\nto 2010, with the exception of those elections during the Great Depression. Here are the data and corresponding linear\\nregression. The estimated parameters of this regression\\nrelate the percent change in House seats for President\\'s party and the unemployment rate. We might wonder, do these data provide strong\\nevidence that the \"true\" linear model has a negative slope? We can frame this investigation using a one-sided\\nstatistical hypothesis test. The null hypothesis is that the true linear\\nmodel has slope (represented by beta_one) of zero. The alternative hypothesis is that the true\\nlinear model has slope of less than zero. The alternative hypothesis would imply the\\nhigher the unemployment, the greater the losses for the President\\'s party in the House of\\nRepresentatives. Just like other point estimates we have seen\\nbefore, we can compute a standard error and test statistic for the point estimate of the slope. This table shows output from statistical software\\nfor the least squares regression line in the unemployment example just introduced. We will generally label the test statistic\\nusing a T, since it follows the t distribution. The entries in the first column represent\\nthe least squares estimates, b_zero and b_one, and the values in the second column correspond\\nto the standard errors of each estimate. We previously used a t-test statistic for\\nhypothesis testing in the context of numerical data. Regression is very similar. In the hypotheses we consider, the null value\\nfor the slope is zero, so we can compute the test statistic using the T (or Z) score formula:\\nthe test statistic equals the estimate minus the null value, divided by the standard error,\\nwhich is negative one point one five in our elections example. The output from statistical software shows\\nthis information, and also provides the p-value of zero point two six one seven, corresponding\\nto a two-sided test. Since we have a one sided test, our p-value\\nwill be half that, which is approximately thirteen percent. Consequently, we do not reject the null hypothesis. That is, the data do not provide strong evidence that a higher unemployment rate has any correspondence with smaller or larger losses for the President\\'s party in the House of Representatives in midterm elections. Before moving on, it is important to note\\nthat if conditions for fitting the regression line do not hold, then the methods presented\\nhere should not be applied. In such cases, the standard error or distribution\\nassumption of the point estimate -- assumed to be normal when applying the t-test statistic\\n-- may not be valid. In this video we were introduced to inference\\nfor parameters in a linear regression, using an example from midterm elections. If you learned something you found interesting,\\nshare this video with a friend and visit openintro.org for more resources.']\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
      "Final Answer:  In regression analysis, we aim to understand the relationship between variables and model their influence on outcomes using a statistical approach. \n",
      "\n",
      "A key aspect of this process involves understanding how to interpret the output from software packages like SPSS, R, or Excel used for performing the analysis. \n",
      "\n",
      "To illustrate this, let's consider a specific example where we analyze data on Midterm elections (the party in power) and unemployment rates: \n",
      "* **Data:** We analyzed historical data from 1898 to 2010 of midterm elections, focusing on how political parties fared during Presidential election years. We aimed to investigate if higher unemployment correlated with losses for the President's party in the House of Representatives.\n",
      "\n",
      "**Statistical Analysis:**  We conducted a linear regression analysis using software like SPSS or R. The analysis estimated the relationship between the percent change in House seats (for the winning party) and unemployment rate, using statistical parameters to measure this connection. \n",
      "\n",
      "* **Regression Output: ** Software provided an estimated slope representing the relationship between these variables. It also offered standard errors and a p-value. These outputs are used for testing hypotheses about the true significance of the regression model.  \n",
      "    \n",
      "**Hypothesis Testing:** The analysis began with a hypothesis test to evaluate if there's evidence of a significant relationship. We formulated the null and alternative hypotheses: \n",
      "\n",
      "* **Null Hypothesis (H0):** There is no connection between unemployment rate and party losses in midterm elections (the slope is zero).  \n",
      "* **Alternative Hypothesis (Ha):** The higher the unemployment rate, the greater the losses for the President's party in the House of Representatives.  \n",
      "\n",
      "We then calculated the t-statistic based on the estimated slope, standard error of the estimate, and null hypothesis. We also used a two-sided test to compute the p-value.\n",
      "\n",
      " **Interpretation:** The result was that we didn't reject the null hypothesis. This means the data doesn't provide strong evidence that higher unemployment rate is strongly correlated with smaller or larger losses for the President's party in the House of Representatives during midterms.   \n",
      "\n",
      "\n",
      "We must always remember that the assumption of normality for t-test statistics may not hold if conditions for fitting the regression line don't hold. \n",
      "\n",
      "Finally, sharing this information can be valuable to help others understand and learn about statistical inference in regression analysis!  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-09-04 18:45:02][DEBUG]: == [Content Researcher] Task output: In regression analysis, we aim to understand the relationship between variables and model their influence on outcomes using a statistical approach. \n",
      "\n",
      "A key aspect of this process involves understanding how to interpret the output from software packages like SPSS, R, or Excel used for performing the analysis. \n",
      "\n",
      "To illustrate this, let's consider a specific example where we analyze data on Midterm elections (the party in power) and unemployment rates: \n",
      "* **Data:** We analyzed historical data from 1898 to 2010 of midterm elections, focusing on how political parties fared during Presidential election years. We aimed to investigate if higher unemployment correlated with losses for the President's party in the House of Representatives.\n",
      "\n",
      "**Statistical Analysis:**  We conducted a linear regression analysis using software like SPSS or R. The analysis estimated the relationship between the percent change in House seats (for the winning party) and unemployment rate, using statistical parameters to measure this connection. \n",
      "\n",
      "* **Regression Output: ** Software provided an estimated slope representing the relationship between these variables. It also offered standard errors and a p-value. These outputs are used for testing hypotheses about the true significance of the regression model.  \n",
      "    \n",
      "**Hypothesis Testing:** The analysis began with a hypothesis test to evaluate if there's evidence of a significant relationship. We formulated the null and alternative hypotheses: \n",
      "\n",
      "* **Null Hypothesis (H0):** There is no connection between unemployment rate and party losses in midterm elections (the slope is zero).  \n",
      "* **Alternative Hypothesis (Ha):** The higher the unemployment rate, the greater the losses for the President's party in the House of Representatives.  \n",
      "\n",
      "We then calculated the t-statistic based on the estimated slope, standard error of the estimate, and null hypothesis. We also used a two-sided test to compute the p-value.\n",
      "\n",
      " **Interpretation:** The result was that we didn't reject the null hypothesis. This means the data doesn't provide strong evidence that higher unemployment rate is strongly correlated with smaller or larger losses for the President's party in the House of Representatives during midterms.   \n",
      "\n",
      "\n",
      "We must always remember that the assumption of normality for t-test statistics may not hold if conditions for fitting the regression line don't hold. \n",
      "\n",
      "Finally, sharing this information can be valuable to help others understand and learn about statistical inference in regression analysis!\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 18:45:02][DEBUG]: == Working Agent: Content Planner\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 18:45:02][INFO]: == Starting Task: Given the user query and relvant content, curate a table of content for the response\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought:  I will generate a table of contents for a response on regression analysis and interpretation. \n",
      "Final Answer: \n",
      "\n",
      "**Table of Contents:**\n",
      "\n",
      "* **Introduction to Regression Analysis** \n",
      "    * Briefly explain the purpose of regression analysis and its role in understanding variable relationships. \n",
      "* **Understanding Statistical Significance** \n",
      "    * Define p-values, t-statistic, confidence intervals, and their importance in hypothesis testing.  \n",
      "* **Regression Model Output Explained**\n",
      "    * Describe the meaning of different elements on the output, including the estimated regression equation, R² value, standard errors, F-statistic, and other significant parameters. \n",
      "* **Hypothesis Testing: A Step-by-Step Guide**\n",
      "   * Explain how to formulate null and alternative hypotheses, calculate t-statistics, and use p-values for hypothesis testing.  \n",
      "    * Provide a clear example with the data from midterm elections to demonstrate this process.  \n",
      "* **Interpreting Regression Results:** \n",
      "     * Discuss the meaning of the estimated slope coefficients in relation to the variables, considering the context of the study (midterm elections and unemployment rates).\n",
      "   * Explain the concept of significance testing (p-value) within the context of the regression model and interpretation. \n",
      "* **Assumptions of Regression Analysis**  \n",
      "    * Explain the assumption of normality, why it matters in hypothesis testing, and how to check for it with the data.\n",
      "* **Practical Applications of Regression Analysis:** \n",
      "   * Discuss real-world examples and applications of regression analysis beyond the midterm elections study. This can cover topics like predicting customer behavior based on demographics or forecasting market trends. \n",
      "* **Conclusion**  \n",
      "    * Briefly summarize the key concepts and takeaways from the discussion.  \n",
      "* **References** (if applicable) \n",
      "   * Provide a list of references used for this response.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-09-04 18:46:04][DEBUG]: == [Content Planner] Task output: **Table of Contents:**\n",
      "\n",
      "* **Introduction to Regression Analysis** \n",
      "    * Briefly explain the purpose of regression analysis and its role in understanding variable relationships. \n",
      "* **Understanding Statistical Significance** \n",
      "    * Define p-values, t-statistic, confidence intervals, and their importance in hypothesis testing.  \n",
      "* **Regression Model Output Explained**\n",
      "    * Describe the meaning of different elements on the output, including the estimated regression equation, R² value, standard errors, F-statistic, and other significant parameters. \n",
      "* **Hypothesis Testing: A Step-by-Step Guide**\n",
      "   * Explain how to formulate null and alternative hypotheses, calculate t-statistics, and use p-values for hypothesis testing.  \n",
      "    * Provide a clear example with the data from midterm elections to demonstrate this process.  \n",
      "* **Interpreting Regression Results:** \n",
      "     * Discuss the meaning of the estimated slope coefficients in relation to the variables, considering the context of the study (midterm elections and unemployment rates).\n",
      "   * Explain the concept of significance testing (p-value) within the context of the regression model and interpretation. \n",
      "* **Assumptions of Regression Analysis**  \n",
      "    * Explain the assumption of normality, why it matters in hypothesis testing, and how to check for it with the data.\n",
      "* **Practical Applications of Regression Analysis:** \n",
      "   * Discuss real-world examples and applications of regression analysis beyond the midterm elections study. This can cover topics like predicting customer behavior based on demographics or forecasting market trends. \n",
      "* **Conclusion**  \n",
      "    * Briefly summarize the key concepts and takeaways from the discussion.  \n",
      "* **References** (if applicable) \n",
      "   * Provide a list of references used for this response.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 18:46:04][DEBUG]: == Working Agent: Content Wrtier\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 18:46:04][INFO]: == Starting Task: Write a detailed and helpful response when provided relevant content and a well-planned table of contents\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I understand.  I need to create a well-structured, detailed explanation of regression analysis that covers all aspects of the table of contents.  I will make sure each section provides clear explanations and real-world examples for understanding. \n",
      "\n",
      "\n",
      "Final Answer: \n",
      "\n",
      "**Introduction to Regression Analysis**\n",
      "\n",
      "Regression analysis is a statistical tool used to understand how variables are related to one another. It helps us predict future values based on existing data by using mathematical equations and models that capture the underlying relationships between variables.  The goal of regression analysis is to model the relationship between independent (predictor) variables and a dependent variable (outcome). \n",
      "\n",
      "**Understanding Statistical Significance**\n",
      "\n",
      "A key aspect of regression analysis involves understanding statistical significance, which helps us determine if observed effects are real or simply due to random chance. Here's how we approach this:\n",
      "\n",
      "* **P-values:**  The p-value represents the probability of observing results as extreme as those obtained from our data, assuming there is no actual relationship between variables. A low p-value (typically below 0.05) indicates that the observed effect is likely statistically significant and suggests that our model accurately reflects a real relationship.\n",
      "* **t-statistic:**  The t-statistic measures how many standard deviations away from zero an estimated regression coefficient lies. It helps us assess the magnitude of the effect size and determine its significance based on the sample data.  \n",
      "* **Confidence Intervals:** These intervals offer a range within which we believe the true population value likely falls. The confidence level (e.g., 95%) reflects our probability that the true value lies within this interval. \n",
      "\n",
      "**Regression Model Output Explained**\n",
      "\n",
      "Understanding regression model output is crucial for interpreting results:\n",
      "   * **Estimated Regression Equation:** This equation expresses the relationship between the dependent variable and one or more independent variables, typically using a linear form like y = b0 + b1x1 + b2x2 +... . \n",
      "   * **R² Value (Coefficient of Determination):**  This value represents the percentage of variance in the dependent variable that is explained by the independent variables. A higher R² suggests better predictive power and model fit.  \n",
      "   * **Standard Errors:** These values provide an estimate of the uncertainty associated with each estimated coefficient, reflecting the reliability of the coefficients' estimates. \n",
      "   * **F-statistic:**  This statistic assesses if there is a statistically significant relationship between independent variables and the dependent variable. A high F-statistic (typically larger than 1) suggests strong evidence for a significant relationship.\n",
      "\n",
      "**Hypothesis Testing: A Step-by-Step Guide**\n",
      "To test hypotheses related to regression models, we employ the following steps:\n",
      "\n",
      "1. **Formulate Null and Alternative Hypotheses:** \n",
      "   * Null Hypothesis (H0): There is no linear relationship between the independent variable(s) and the dependent variable (no effect).\n",
      "   * Alternative Hypothesis (Ha): There is a linear relationship between the independent variables and the dependent variable (a significant effect).\n",
      "\n",
      "2. **Calculate the t-statistic:**  This measures how many standard deviations away from zero our estimated coefficients lie. \n",
      "\n",
      "3. **Calculate p-value:** This represents the probability of observing the data under the assumption that there is no true relationship between the independent and dependent variables. A low p-value (typically below 0.05) suggests strong evidence against the null hypothesis. \n",
      " \n",
      "4. **Compare p-values to Significance Level:**  We compare our calculated p-value with a predetermined significance level (e.g., 0.05). If the p-value is less than the significance level, we reject the null hypothesis in favor of the alternative hypothesis.\n",
      "\n",
      "**Example: Midterm Elections and Unemployment Rate**\n",
      "Let's apply this to an example of midterm elections and unemployment rate:  We may hypothesize that a higher unemployment rate leads to a decrease in voter turnout for midterm elections. \n",
      "\n",
      "* **Hypotheses:**\n",
      "    * **Null Hypothesis (H0):** There is no relationship between unemployment rates and voter turnout for midterm elections.\n",
      "    * **Alternative Hypothesis (Ha):** There is a negative correlation between unemployment rate and voter turnout in midterm elections.  (i.e., Higher unemployment leads to lower voter turnout).\n",
      "\n",
      "* **Calculation:** A regression analysis model can estimate the relationship, incorporating factors such as income, education levels, political party affiliations, and geographic location. \n",
      "* **Hypothesis Testing:** The p-value from the regression analysis helps determine the significance of this relationship. For example, if the p-value is low (e.g., <0.05), we would reject the null hypothesis, suggesting a statistically significant relationship between unemployment and voter turnout in the midterm elections.\n",
      "\n",
      "\n",
      "**Interpreting Regression Results: A Deeper Look at Significance**\n",
      "\n",
      "The estimated coefficients represent the change in the dependent variable for each unit increase in an independent variable, holding all other variables constant. We can interpret these effects based on their significance (p-value). \n",
      "   * **Significance Level:**  For example, if the p-value is less than 0.05, we would reject the null hypothesis and say that the relationship is statistically significant. \n",
      "   * **Confidence Interval:** This helps in understanding the range of values for the estimated coefficient with a certain level of confidence (e.g., 95%).\n",
      "\n",
      "**Assumptions of Regression Analysis**\n",
      "Regression analysis relies on various assumptions for its validity:\n",
      "   * **Linearity:** The relationship between variables should be linear, where changes in one variable lead to proportional changes in another. This assumption ensures the model can accurately reflect the relationship and makes it possible to calculate coefficients. \n",
      "   * **Independence of observations:** Each observation in the data set should be independent of others (no systematic influence) on the dependent variable.  This is a key assumption for the statistical validity of the analysis. \n",
      "   * **Normality of residuals:** The distribution of the residuals (errors between actual and predicted values) should be approximately normal. This helps ensure that the model assumptions hold, as normality impacts confidence intervals and hypothesis testing.\n",
      "\n",
      "\n",
      "**Practical Applications of Regression Analysis**\n",
      "\n",
      "Beyond studying elections:\n",
      "\n",
      "* **Predicting Customer Behavior:**  Regressions can predict customer behavior based on demographics or past purchases to improve marketing strategies.\n",
      "   \n",
      "* **Forecasting Market Trends:**  Regression analysis helps forecast market trends by analyzing historical data to identify patterns and relationships between variables, such as supply and demand. \n",
      "\n",
      "\n",
      "\n",
      "**Conclusion: Summary of Key Concepts**\n",
      "\n",
      "Regression analysis is a valuable tool for understanding how variables are related and making accurate predictions based on existing data. This powerful method involves hypothesis testing, interpretation of model output, and statistical assumptions that must be considered to ensure the accuracy of the results.\n",
      "\n",
      "\n",
      " **References:**  (Insert relevant references here if required) \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-09-04 18:47:38][DEBUG]: == [Content Wrtier] Task output: **Introduction to Regression Analysis**\n",
      "\n",
      "Regression analysis is a statistical tool used to understand how variables are related to one another. It helps us predict future values based on existing data by using mathematical equations and models that capture the underlying relationships between variables.  The goal of regression analysis is to model the relationship between independent (predictor) variables and a dependent variable (outcome). \n",
      "\n",
      "**Understanding Statistical Significance**\n",
      "\n",
      "A key aspect of regression analysis involves understanding statistical significance, which helps us determine if observed effects are real or simply due to random chance. Here's how we approach this:\n",
      "\n",
      "* **P-values:**  The p-value represents the probability of observing results as extreme as those obtained from our data, assuming there is no actual relationship between variables. A low p-value (typically below 0.05) indicates that the observed effect is likely statistically significant and suggests that our model accurately reflects a real relationship.\n",
      "* **t-statistic:**  The t-statistic measures how many standard deviations away from zero an estimated regression coefficient lies. It helps us assess the magnitude of the effect size and determine its significance based on the sample data.  \n",
      "* **Confidence Intervals:** These intervals offer a range within which we believe the true population value likely falls. The confidence level (e.g., 95%) reflects our probability that the true value lies within this interval. \n",
      "\n",
      "**Regression Model Output Explained**\n",
      "\n",
      "Understanding regression model output is crucial for interpreting results:\n",
      "   * **Estimated Regression Equation:** This equation expresses the relationship between the dependent variable and one or more independent variables, typically using a linear form like y = b0 + b1x1 + b2x2 +... . \n",
      "   * **R² Value (Coefficient of Determination):**  This value represents the percentage of variance in the dependent variable that is explained by the independent variables. A higher R² suggests better predictive power and model fit.  \n",
      "   * **Standard Errors:** These values provide an estimate of the uncertainty associated with each estimated coefficient, reflecting the reliability of the coefficients' estimates. \n",
      "   * **F-statistic:**  This statistic assesses if there is a statistically significant relationship between independent variables and the dependent variable. A high F-statistic (typically larger than 1) suggests strong evidence for a significant relationship.\n",
      "\n",
      "**Hypothesis Testing: A Step-by-Step Guide**\n",
      "To test hypotheses related to regression models, we employ the following steps:\n",
      "\n",
      "1. **Formulate Null and Alternative Hypotheses:** \n",
      "   * Null Hypothesis (H0): There is no linear relationship between the independent variable(s) and the dependent variable (no effect).\n",
      "   * Alternative Hypothesis (Ha): There is a linear relationship between the independent variables and the dependent variable (a significant effect).\n",
      "\n",
      "2. **Calculate the t-statistic:**  This measures how many standard deviations away from zero our estimated coefficients lie. \n",
      "\n",
      "3. **Calculate p-value:** This represents the probability of observing the data under the assumption that there is no true relationship between the independent and dependent variables. A low p-value (typically below 0.05) suggests strong evidence against the null hypothesis. \n",
      " \n",
      "4. **Compare p-values to Significance Level:**  We compare our calculated p-value with a predetermined significance level (e.g., 0.05). If the p-value is less than the significance level, we reject the null hypothesis in favor of the alternative hypothesis.\n",
      "\n",
      "**Example: Midterm Elections and Unemployment Rate**\n",
      "Let's apply this to an example of midterm elections and unemployment rate:  We may hypothesize that a higher unemployment rate leads to a decrease in voter turnout for midterm elections. \n",
      "\n",
      "* **Hypotheses:**\n",
      "    * **Null Hypothesis (H0):** There is no relationship between unemployment rates and voter turnout for midterm elections.\n",
      "    * **Alternative Hypothesis (Ha):** There is a negative correlation between unemployment rate and voter turnout in midterm elections.  (i.e., Higher unemployment leads to lower voter turnout).\n",
      "\n",
      "* **Calculation:** A regression analysis model can estimate the relationship, incorporating factors such as income, education levels, political party affiliations, and geographic location. \n",
      "* **Hypothesis Testing:** The p-value from the regression analysis helps determine the significance of this relationship. For example, if the p-value is low (e.g., <0.05), we would reject the null hypothesis, suggesting a statistically significant relationship between unemployment and voter turnout in the midterm elections.\n",
      "\n",
      "\n",
      "**Interpreting Regression Results: A Deeper Look at Significance**\n",
      "\n",
      "The estimated coefficients represent the change in the dependent variable for each unit increase in an independent variable, holding all other variables constant. We can interpret these effects based on their significance (p-value). \n",
      "   * **Significance Level:**  For example, if the p-value is less than 0.05, we would reject the null hypothesis and say that the relationship is statistically significant. \n",
      "   * **Confidence Interval:** This helps in understanding the range of values for the estimated coefficient with a certain level of confidence (e.g., 95%).\n",
      "\n",
      "**Assumptions of Regression Analysis**\n",
      "Regression analysis relies on various assumptions for its validity:\n",
      "   * **Linearity:** The relationship between variables should be linear, where changes in one variable lead to proportional changes in another. This assumption ensures the model can accurately reflect the relationship and makes it possible to calculate coefficients. \n",
      "   * **Independence of observations:** Each observation in the data set should be independent of others (no systematic influence) on the dependent variable.  This is a key assumption for the statistical validity of the analysis. \n",
      "   * **Normality of residuals:** The distribution of the residuals (errors between actual and predicted values) should be approximately normal. This helps ensure that the model assumptions hold, as normality impacts confidence intervals and hypothesis testing.\n",
      "\n",
      "\n",
      "**Practical Applications of Regression Analysis**\n",
      "\n",
      "Beyond studying elections:\n",
      "\n",
      "* **Predicting Customer Behavior:**  Regressions can predict customer behavior based on demographics or past purchases to improve marketing strategies.\n",
      "   \n",
      "* **Forecasting Market Trends:**  Regression analysis helps forecast market trends by analyzing historical data to identify patterns and relationships between variables, such as supply and demand. \n",
      "\n",
      "\n",
      "\n",
      "**Conclusion: Summary of Key Concepts**\n",
      "\n",
      "Regression analysis is a valuable tool for understanding how variables are related and making accurate predictions based on existing data. This powerful method involves hypothesis testing, interpretation of model output, and statistical assumptions that must be considered to ensure the accuracy of the results.\n",
      "\n",
      "\n",
      " **References:**  (Insert relevant references here if required)\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.kickoff(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Introduction to Regression Analysis**\n",
       "\n",
       "Regression analysis is a statistical tool used to understand how variables are related to one another. It helps us predict future values based on existing data by using mathematical equations and models that capture the underlying relationships between variables.  The goal of regression analysis is to model the relationship between independent (predictor) variables and a dependent variable (outcome). \n",
       "\n",
       "**Understanding Statistical Significance**\n",
       "\n",
       "A key aspect of regression analysis involves understanding statistical significance, which helps us determine if observed effects are real or simply due to random chance. Here's how we approach this:\n",
       "\n",
       "* **P-values:**  The p-value represents the probability of observing results as extreme as those obtained from our data, assuming there is no actual relationship between variables. A low p-value (typically below 0.05) indicates that the observed effect is likely statistically significant and suggests that our model accurately reflects a real relationship.\n",
       "* **t-statistic:**  The t-statistic measures how many standard deviations away from zero an estimated regression coefficient lies. It helps us assess the magnitude of the effect size and determine its significance based on the sample data.  \n",
       "* **Confidence Intervals:** These intervals offer a range within which we believe the true population value likely falls. The confidence level (e.g., 95%) reflects our probability that the true value lies within this interval. \n",
       "\n",
       "**Regression Model Output Explained**\n",
       "\n",
       "Understanding regression model output is crucial for interpreting results:\n",
       "   * **Estimated Regression Equation:** This equation expresses the relationship between the dependent variable and one or more independent variables, typically using a linear form like y = b0 + b1x1 + b2x2 +... . \n",
       "   * **R² Value (Coefficient of Determination):**  This value represents the percentage of variance in the dependent variable that is explained by the independent variables. A higher R² suggests better predictive power and model fit.  \n",
       "   * **Standard Errors:** These values provide an estimate of the uncertainty associated with each estimated coefficient, reflecting the reliability of the coefficients' estimates. \n",
       "   * **F-statistic:**  This statistic assesses if there is a statistically significant relationship between independent variables and the dependent variable. A high F-statistic (typically larger than 1) suggests strong evidence for a significant relationship.\n",
       "\n",
       "**Hypothesis Testing: A Step-by-Step Guide**\n",
       "To test hypotheses related to regression models, we employ the following steps:\n",
       "\n",
       "1. **Formulate Null and Alternative Hypotheses:** \n",
       "   * Null Hypothesis (H0): There is no linear relationship between the independent variable(s) and the dependent variable (no effect).\n",
       "   * Alternative Hypothesis (Ha): There is a linear relationship between the independent variables and the dependent variable (a significant effect).\n",
       "\n",
       "2. **Calculate the t-statistic:**  This measures how many standard deviations away from zero our estimated coefficients lie. \n",
       "\n",
       "3. **Calculate p-value:** This represents the probability of observing the data under the assumption that there is no true relationship between the independent and dependent variables. A low p-value (typically below 0.05) suggests strong evidence against the null hypothesis. \n",
       " \n",
       "4. **Compare p-values to Significance Level:**  We compare our calculated p-value with a predetermined significance level (e.g., 0.05). If the p-value is less than the significance level, we reject the null hypothesis in favor of the alternative hypothesis.\n",
       "\n",
       "**Example: Midterm Elections and Unemployment Rate**\n",
       "Let's apply this to an example of midterm elections and unemployment rate:  We may hypothesize that a higher unemployment rate leads to a decrease in voter turnout for midterm elections. \n",
       "\n",
       "* **Hypotheses:**\n",
       "    * **Null Hypothesis (H0):** There is no relationship between unemployment rates and voter turnout for midterm elections.\n",
       "    * **Alternative Hypothesis (Ha):** There is a negative correlation between unemployment rate and voter turnout in midterm elections.  (i.e., Higher unemployment leads to lower voter turnout).\n",
       "\n",
       "* **Calculation:** A regression analysis model can estimate the relationship, incorporating factors such as income, education levels, political party affiliations, and geographic location. \n",
       "* **Hypothesis Testing:** The p-value from the regression analysis helps determine the significance of this relationship. For example, if the p-value is low (e.g., <0.05), we would reject the null hypothesis, suggesting a statistically significant relationship between unemployment and voter turnout in the midterm elections.\n",
       "\n",
       "\n",
       "**Interpreting Regression Results: A Deeper Look at Significance**\n",
       "\n",
       "The estimated coefficients represent the change in the dependent variable for each unit increase in an independent variable, holding all other variables constant. We can interpret these effects based on their significance (p-value). \n",
       "   * **Significance Level:**  For example, if the p-value is less than 0.05, we would reject the null hypothesis and say that the relationship is statistically significant. \n",
       "   * **Confidence Interval:** This helps in understanding the range of values for the estimated coefficient with a certain level of confidence (e.g., 95%).\n",
       "\n",
       "**Assumptions of Regression Analysis**\n",
       "Regression analysis relies on various assumptions for its validity:\n",
       "   * **Linearity:** The relationship between variables should be linear, where changes in one variable lead to proportional changes in another. This assumption ensures the model can accurately reflect the relationship and makes it possible to calculate coefficients. \n",
       "   * **Independence of observations:** Each observation in the data set should be independent of others (no systematic influence) on the dependent variable.  This is a key assumption for the statistical validity of the analysis. \n",
       "   * **Normality of residuals:** The distribution of the residuals (errors between actual and predicted values) should be approximately normal. This helps ensure that the model assumptions hold, as normality impacts confidence intervals and hypothesis testing.\n",
       "\n",
       "\n",
       "**Practical Applications of Regression Analysis**\n",
       "\n",
       "Beyond studying elections:\n",
       "\n",
       "* **Predicting Customer Behavior:**  Regressions can predict customer behavior based on demographics or past purchases to improve marketing strategies.\n",
       "   \n",
       "* **Forecasting Market Trends:**  Regression analysis helps forecast market trends by analyzing historical data to identify patterns and relationships between variables, such as supply and demand. \n",
       "\n",
       "\n",
       "\n",
       "**Conclusion: Summary of Key Concepts**\n",
       "\n",
       "Regression analysis is a valuable tool for understanding how variables are related and making accurate predictions based on existing data. This powerful method involves hypothesis testing, interpretation of model output, and statistical assumptions that must be considered to ensure the accuracy of the results.\n",
       "\n",
       "\n",
       " **References:**  (Insert relevant references here if required)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result.raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traversaal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
